# -*- coding: utf-8 -*-
"""Code_Implementation_Fine_tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EHWeSe-f2GWfHXPEXiQmkGZ_Vj3K4ZAj
"""

# Install dataset library
!pip install datasets

# Importing necessarry libraries
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model
import torch

"""
Fine-Tuning DistilGPT-2 with LoRA on the PubMedQA Dataset

This script fine-tunes the DistilGPT-2 model using LoRA (Low-Rank Adaptation)
for medical question-answering tasks using the PubMedQA dataset.

Steps:
1. Load and preprocess the dataset
2. Initialize the tokenizer and set padding token
3. Define a tokenization function
4. Load the pretrained model and apply LoRA configuration
5. Define training arguments
6. Train the model using Hugging Face's Trainer API
7. Save the fine-tuned model for later use
8. Load Base and Fine-Tuned Models for Evaluation


"""

# Step 1: Load and preprocess dataset
dataset = load_dataset("pubmed_qa", "pqa_labeled")

# Print a sample from the dataset
sample = dataset["train"][0]
print(sample)

# Split dataset into training and test sets
dataset = dataset["train"].train_test_split(test_size=0.2)
train_dataset = dataset["train"]
test_dataset = dataset["test"]

# step2 :Load tokenizer and set padding token
tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
tokenizer.pad_token = tokenizer.eos_token  # Set padding token

# Step3 : Define a function for tokenizing input text
def tokenize_function(examples):
    """
    Tokenizes input text by concatenating the question and long answer,
    applying truncation and padding to a fixed max length of 512 tokens.
    """
    return tokenizer(examples["question"], text_pair=examples["long_answer"], truncation=True, padding="max_length", max_length=512)

# Apply tokenization to the dataset
train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

# Define data collator for handling labels
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # Set to False for causal language modeling
)

# Step 4: Load Pretrained Model and Apply LoRA
model = AutoModelForCausalLM.from_pretrained("distilgpt2")
lora_config = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.1, bias="none", task_type="CAUSAL_LM") # Rank parameter # Task type for causal language modeling
model = get_peft_model(model, lora_config) # Apply LoRA modifications to the model

# Step 5: Define Training Arguments
training_args = TrainingArguments(
    output_dir="./results", # Directory to save model checkpoints
    evaluation_strategy="epoch", #  Evaluate model at the end of each epoch
    learning_rate=5e-5, # Learning rate for optimization
    per_device_train_batch_size=1,#Training batch size per device
    per_device_eval_batch_size=1, #Evaluation batch size per device
    num_train_epochs=3, # Number of training epochs
    weight_decay=0.01, # Weight decay for regularization
    logging_dir="./logs", # Directory for logging training metrics
    save_strategy="epoch", #Save model at the end of each epoch
    report_to="none",
)

#Train the Fine-Tuned Model
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,  # Include data collator to handle labels
)

# Step6 :Train the model
trainer.train()
# Both training loss and validation loss are decreasing, indicating that the model is learning effectively.



# Step7 : Save Model
model.save_pretrained("./fine_tuned_model")
tokenizer.save_pretrained("./fine_tuned_model")

print("Fine-tuning complete!")

# Step 8: Load Base and Fine-Tuned Models for Evaluation
base_model = AutoModelForCausalLM.from_pretrained("distilgpt2")
fine_tuned_model = AutoModelForCausalLM.from_pretrained("./fine_tuned_model")
fine_tuned_tokenizer = AutoTokenizer.from_pretrained("./fine_tuned_model")

import numpy as np
# Performing Perplexity Evaluation
def compute_perplexity(model, tokenizer, dataset):
    """
    Computes the perplexity of a given model on the test dataset.
    """
    model.eval()
    losses = []
    for sample in dataset.select(range(100)):  # Limit evaluation to 100 samples for efficiency
        inputs = tokenizer(sample["question"], return_tensors="pt", truncation=True, padding=True, max_length=512)
        with torch.no_grad():
            outputs = model(**inputs, labels=inputs["input_ids"])
            loss = outputs.loss.item()
            losses.append(loss)
    return np.exp(np.mean(losses))  # Perplexity computation

# Compute Perplexity for base and fine-tuned models
perplexity_base = compute_perplexity(base_model, tokenizer, test_dataset)
perplexity_fine_tuned = compute_perplexity(fine_tuned_model, fine_tuned_tokenizer, test_dataset)

print(f"Perplexity (Base Model): {perplexity_base}")
print(f"Perplexity (Fine-Tuned Model): {perplexity_fine_tuned}")

#Base Model Perplexity: 315.66 (Higher = Less Fluent)
#Fine-Tuned Model Perplexity: 125.80 (Lower = More Fluent)
#perplexity scores show a significant improvement after fine-tuning
#The fine-tuned model has learned from the medical dataset (PubMedQA) and is now better at predicting the next words in a sequence.
#Lower perplexity suggests that the model generates more coherent, fluent, and relevant responses.



# Generate a response to a medical question
input_text = "Question: What are the symptoms of diabetes? Answer:"
input_ids = fine_tuned_tokenizer.encode(input_text, return_tensors="pt")

# Generate the answer
output = fine_tuned_model.generate(input_ids, max_length=100, num_return_sequences=1,repetition_penalty=1.5)
generated_text = fine_tuned_tokenizer.decode(output[0], skip_special_tokens=True)
print("Generated Answer:", generated_text)

pip install evaluate datasets

pip install rouge_score

# Perform BLEU and ROUGE Evaluation
from evaluate import load
# Load Fine Tuned Model for Evaluation
fine_tuned_model = AutoModelForCausalLM.from_pretrained("./fine_tuned_model")
fine_tuned_tokenizer = AutoTokenizer.from_pretrained("./fine_tuned_model")

bleu = load("bleu")
rouge = load("rouge")

def evaluate_bleu_rouge(model, tokenizer, dataset, num_samples=100):
    """
    Computes BLEU and ROUGE scores for a given model on the test dataset.
    """
    predictions, references = [], []
    for sample in dataset.select(range(num_samples)):
        input_ids = tokenizer(sample["question"], return_tensors="pt", truncation=True, padding=True, max_length=512).input_ids
        output = model.generate(input_ids, max_length=100, num_return_sequences=1, pad_token_id=tokenizer.pad_token_id)
        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
        predictions.append(generated_text)
        references.append([sample["long_answer"]])

    bleu_score = bleu.compute(predictions=predictions, references=references)
    rouge_score = rouge.compute(predictions=predictions, references=references)

    return bleu_score, rouge_score

# Compute BLEU and ROUGE scores before and after fine-tuning
bleu_score_before, rouge_score_before = evaluate_bleu_rouge(base_model, tokenizer, test_dataset)
bleu_score_after, rouge_score_after = evaluate_bleu_rouge(fine_tuned_model, fine_tuned_tokenizer, test_dataset)

print(f"BLEU Score Before Fine-Tuning: {bleu_score_before}")
print(f"ROUGE Score Before Fine-Tuning: {rouge_score_before}")
print(f"BLEU Score After Fine-Tuning: {bleu_score_after}")
print(f"ROUGE Score After Fine-Tuning: {rouge_score_after}")

#Analysis of Results before and after finetuning:
#Perplexity Before: 289.5865947225306 and after: 125.887103806811
#BLEU Score Before: 0.0167 and After: 0.0263
#ROUGE-1 Before: 0.249 and After: 0.184
#ROUGE-2 Before: 0.100 and After: 0.053

